{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 2nd Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "token = \"token\"\n",
    "\n",
    "def get_popular_java_repos(total_repos):\n",
    "    url = \"https://api.github.com/graphql\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    all_repos = []\n",
    "    end_cursor = None\n",
    "    max_retries = 5\n",
    "    retry_delay = 5\n",
    "\n",
    "    while len(all_repos) < total_repos:\n",
    "        remaining_repos = total_repos - len(all_repos)\n",
    "        first = min(remaining_repos, 10)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "            search(query: \"language:Java stars:>1\", type: REPOSITORY, first: {first}, after: {f'\"{end_cursor}\"' if end_cursor else 'null'}) {{\n",
    "                edges {{\n",
    "                    node {{\n",
    "                        ... on Repository {{\n",
    "                          name\n",
    "                          url\n",
    "                          stargazers {{\n",
    "                            totalCount\n",
    "                          }}\n",
    "                          owner {{\n",
    "                            login\n",
    "                          }}\n",
    "                          createdAt\n",
    "                          pushedAt\n",
    "                          releases {{\n",
    "                            totalCount\n",
    "                          }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                pageInfo {{\n",
    "                    endCursor\n",
    "                    hasNextPage\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for attempt in range(max_retries):\n",
    "                response = requests.post(url, json={'query': query}, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    search_results = data.get(\"data\", {}).get(\"search\", {})\n",
    "                    if search_results:\n",
    "                        edges = search_results.get(\"edges\", [])\n",
    "                        all_repos.extend(edges)\n",
    "                        end_cursor = search_results.get(\"pageInfo\", {}).get(\"endCursor\")\n",
    "                        if not search_results.get(\"pageInfo\", {}).get(\"hasNextPage\", False):\n",
    "                            return all_repos\n",
    "                        print(f\"Progress: Found {len(all_repos)} repositories.\")\n",
    "                    break\n",
    "                elif response.status_code in [502, 503, 504]:\n",
    "                    print(f\"Error {response.status_code}: Attempt {attempt + 1} of {max_retries}. Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "                elif response.status_code == 429:\n",
    "                    print(\"Rate limit exceeded. Waiting for the reset time.\")\n",
    "                    reset_time = int(response.headers.get('X-RateLimit-Reset', time.time()))\n",
    "                    wait_time = max(reset_time - time.time(), 0)\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to fetch repositories: {response.status_code}\")\n",
    "            else:\n",
    "                raise Exception(\"Max retries reached, aborting.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    return all_repos\n",
    "\n",
    "\n",
    "def calculate_quality_metrics(class_file):\n",
    "    if not os.path.exists(class_file):\n",
    "        print(f\"Arquivo {class_file} não existe.\")\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    if os.stat(class_file).st_size == 0:\n",
    "        print(f\"Arquivo {class_file} está vazio.\")\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    total_cbo = total_dit = total_lcom = loc_total = repo_count = 0\n",
    "    with open(class_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            total_cbo += int(row.get('cbo', 0))\n",
    "            total_dit += int(row.get('dit', 0))\n",
    "            total_lcom += int(row.get('lcom', 0))\n",
    "            loc_total += int(row.get('loc', 0))\n",
    "            repo_count += 1\n",
    "    if repo_count == 0:\n",
    "        print(f\"Nenhum dado encontrado no arquivo {class_file}.\")\n",
    "        return -1, -1, -1, -1\n",
    "    return total_cbo, total_dit, total_lcom, loc_total\n",
    "\n",
    "\n",
    "\n",
    "def export_to_csv(repos, filename=\"repos.csv\"):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Name\", \"URL\", \"Stars\", \"Quantity of Releases\", \"Year Age\", \"Created At\", \"Pushed At\"])\n",
    "        for repo_edge in repos:\n",
    "            repo = repo_edge[\"node\"]\n",
    "            created_at = datetime.datetime.strptime(repo['createdAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            age_years = (datetime.datetime.now() - created_at).days / 365.25\n",
    "            writer.writerow([\n",
    "                repo[\"name\"], repo[\"url\"], \n",
    "                repo[\"stargazers\"][\"totalCount\"],\n",
    "                repo['releases']['totalCount'],\n",
    "                age_years,\n",
    "                repo[\"createdAt\"], \n",
    "                repo[\"pushedAt\"]\n",
    "            ])\n",
    "    print(f\"Dados exportados para {filename} com sucesso.\")\n",
    "\n",
    "\n",
    "def calculate_process_metrics(repos):\n",
    "    process_metrics = []\n",
    "    for repo in repos:\n",
    "        repo_node = repo['node']\n",
    "        created_at = datetime.datetime.strptime(repo_node['createdAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "        age_years = (datetime.datetime.now() - created_at).days / 365.25\n",
    "        releases_count = repo_node['releases']['totalCount']\n",
    "        stargazers_count = repo_node['stargazers']['totalCount']\n",
    "        \n",
    "        process_metrics.append({\n",
    "            'name': repo_node['name'],\n",
    "            \"stars\": stargazers_count,\n",
    "            'age_years': age_years,\n",
    "            'releases_count': releases_count,\n",
    "            'stargazers_count': stargazers_count\n",
    "        })\n",
    "    return process_metrics\n",
    "\n",
    "\n",
    "def merge_metrics(process_metrics, quality_metrics):\n",
    "    merged_data = []\n",
    "    for process_metric in process_metrics:\n",
    "        name = process_metric['name']\n",
    "        quality_metric = quality_metrics.get(name, {})\n",
    "        merged_data.append({\n",
    "            'name': name,\n",
    "            'popularity': process_metric['stars'],\n",
    "            'maturity': process_metric['age_years'],\n",
    "            \"activity\": process_metric['releases_count'],\n",
    "            'loc_total': quality_metric['loc_total'],\n",
    "            'avg_cbo': quality_metric['avg_cbo'],\n",
    "            'avg_dit': quality_metric['avg_dit'],\n",
    "            'avg_lcom': quality_metric['avg_lcom']\n",
    "        })\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def clone_repo(repo_url, repo_name, repo_owner, failed_repos=[]):\n",
    "    try:\n",
    "        repo_path = f\"./repos/{repo_owner}_{repo_name}\"\n",
    "        ck_output = f\"./ck_output/{repo_owner}_{repo_name}\"\n",
    "        os.makedirs(os.path.dirname(repo_path), exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(ck_output):\n",
    "            print(f\"Skipping {repo_owner}/{repo_name} as it was already cloned.\")\n",
    "            return False\n",
    "        subprocess.run([\n",
    "            \"git\", \"clone\", \"--single-branch\", \"--no-tags\", \"--depth\", \"1\", repo_url, repo_path\n",
    "        ], check=True)\n",
    "        print(f\"Cloned {repo_owner}/{repo_name} successfully ({repo_url}).\")\n",
    "        return True \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cloning {repo_owner}/{repo_name} -> {repo_url}: {e}.\")\n",
    "        failed_repos.append((repo_name, repo_url, repo_owner))\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_result_dir(repo_path):\n",
    "  try:\n",
    "    output_dir = f\"./ck_output/{repo_path}/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "  except Exception as e:\n",
    "    print(f\"Error running the creation directories: {e}\")\n",
    "\n",
    "\n",
    "def remove_repo(repo_path):\n",
    "  try:\n",
    "    subprocess.run([\n",
    "      \"rm\", \"-rf\", f\"./repos/{repo_path}\"\n",
    "    ], check=True)\n",
    "    print(f\"Removed {repo_path} successfully.\")\n",
    "  except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error removing {repo_path}: {e}\")\n",
    "    \n",
    "\n",
    "def remove_all_folders():\n",
    "  try:\n",
    "    subprocess.run([\n",
    "      \"rm\", \"-rf\", \"./repos\"\n",
    "    ], check=True)\n",
    "    subprocess.run([\n",
    "      \"rm\", \"-rf\", \"./ck_output\"\n",
    "    ], check=True)\n",
    "    print(f\"Removed all folders successfully.\")\n",
    "  except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error removing all folders: {e}\")\n",
    "\n",
    "\n",
    "def run_ck_on_repo(repo_path):\n",
    "    try:\n",
    "        repo_path_url = f\"./repos/{repo_path}/\"\n",
    "        output_dir = f\"./ck_output/{repo_path}/\"\n",
    "        if os.path.exists(output_dir) and os.path.exists(f\"{output_dir}/class.csv\"):\n",
    "            print(f\"Skipping CK on {repo_path} as it was already run.\")\n",
    "            return\n",
    "        subprocess.run([\n",
    "            \"java\", \"-jar\", \"./ck.jar\",\n",
    "            repo_path_url, \"true\", \"0\", \"true\", output_dir\n",
    "        ], check=True)\n",
    "        print(f\"Ran CK on {repo_path} successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running CK on {repo_path}: {e}\")\n",
    "\n",
    "\n",
    "def retrieve_final_metrics(quantity=1000):\n",
    "    repos = get_popular_java_repos(quantity)\n",
    "    export_to_csv(repos)\n",
    "\n",
    "    quality_metrics = {}\n",
    "    for repo in repos:\n",
    "        repo_url = repo[\"node\"][\"url\"]\n",
    "        repo_name = repo[\"node\"][\"name\"]\n",
    "        repo_owner = repo[\"node\"][\"owner\"][\"login\"]\n",
    "        \n",
    "        repo_owner_name = f\"{repo_owner}_{repo_name}\"\n",
    "        create_result_dir(repo_owner_name)\n",
    "\n",
    "        failed_repos = []\n",
    "        clone_repo(repo_url, repo_name, repo_owner, failed_repos=failed_repos)\n",
    "        if failed_repos:\n",
    "            print(\"\\nRetrying failed repositories...\\n\")\n",
    "            retry_failed_repos = []\n",
    "            for repo_name, repo_url, repo_owner in failed_repos:\n",
    "                clone_repo(repo_url, repo_name, repo_owner, failed_repos=retry_failed_repos)\n",
    "\n",
    "            if retry_failed_repos:\n",
    "                print(\"\\nThe following repositories could not be cloned even after retries:\")\n",
    "                for repo_name, repo_url, repo_owner in retry_failed_repos:\n",
    "                    print(f\"Failed: {repo_name} ({repo_url})\")\n",
    "            else:\n",
    "                print(\"\\nAll previously failed repositories were successfully cloned.\")\n",
    "        run_ck_on_repo(repo_owner_name)\n",
    "        remove_repo(repo_owner_name)\n",
    "    \n",
    "    for repo in repos:\n",
    "        repo_owner = repo['node']['owner']['login']\n",
    "        repo_name = repo['node']['name']\n",
    "        class_file = f\"./ck_output/{repo_owner}_{repo_name}/class.csv\"\n",
    "        quality_metrics_result = calculate_quality_metrics(class_file)\n",
    "        if quality_metrics_result:\n",
    "            avg_cbo, avg_dit, avg_lcom, loc_total = quality_metrics_result\n",
    "            quality_metrics[repo_name] = {\n",
    "                'avg_cbo': avg_cbo,\n",
    "                'avg_dit': avg_dit,\n",
    "                'avg_lcom': avg_lcom,\n",
    "                'loc_total': loc_total\n",
    "            }\n",
    "            print(quality_metrics[repo_name])\n",
    "        else:\n",
    "            print(f\"Ignoring {repo_name} due to missing or empty quality metrics.\")\n",
    "\n",
    "    process_metrics = calculate_process_metrics(repos)\n",
    "    final_metrics = merge_metrics(process_metrics, quality_metrics)\n",
    "    return final_metrics, repos\n",
    "\n",
    "\n",
    "def summarize_data(df, columns):\n",
    "    summary = {}\n",
    "    for col in columns:\n",
    "        summary[col] = {\n",
    "            'median': np.median(df[col]),\n",
    "            'mean': np.mean(df[col]),\n",
    "            'std': np.std(df[col])\n",
    "        }\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "final_metrics, repos = retrieve_final_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics_df = pd.DataFrame(final_metrics)\n",
    "metrics_columns = ['popularity', 'maturity', 'activity', 'loc_total', 'avg_cbo', 'avg_dit', 'avg_lcom']\n",
    "summary_df = summarize_data(final_metrics_df, metrics_columns)\n",
    "print(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
