{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 2nd Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: Found 10 repositories.\n",
      "Progress: Found 20 repositories.\n",
      "Progress: Found 30 repositories.\n",
      "Progress: Found 40 repositories.\n",
      "Progress: Found 50 repositories.\n",
      "Progress: Found 60 repositories.\n",
      "Progress: Found 70 repositories.\n",
      "Progress: Found 80 repositories.\n",
      "Progress: Found 90 repositories.\n",
      "Progress: Found 100 repositories.\n",
      "Progress: Found 110 repositories.\n",
      "Progress: Found 120 repositories.\n",
      "Progress: Found 130 repositories.\n",
      "Progress: Found 140 repositories.\n",
      "Progress: Found 150 repositories.\n",
      "Progress: Found 160 repositories.\n",
      "Progress: Found 170 repositories.\n",
      "Progress: Found 180 repositories.\n",
      "Progress: Found 190 repositories.\n",
      "Progress: Found 200 repositories.\n",
      "Progress: Found 210 repositories.\n",
      "Progress: Found 220 repositories.\n",
      "Progress: Found 230 repositories.\n",
      "Progress: Found 240 repositories.\n",
      "Progress: Found 250 repositories.\n",
      "Progress: Found 260 repositories.\n",
      "Progress: Found 270 repositories.\n",
      "Progress: Found 280 repositories.\n",
      "Progress: Found 290 repositories.\n",
      "Progress: Found 300 repositories.\n",
      "Progress: Found 310 repositories.\n",
      "Progress: Found 320 repositories.\n",
      "Progress: Found 330 repositories.\n",
      "Progress: Found 340 repositories.\n",
      "Progress: Found 350 repositories.\n",
      "Progress: Found 360 repositories.\n",
      "Progress: Found 370 repositories.\n",
      "Progress: Found 380 repositories.\n",
      "Progress: Found 390 repositories.\n",
      "Progress: Found 400 repositories.\n",
      "Progress: Found 410 repositories.\n",
      "Progress: Found 420 repositories.\n",
      "Progress: Found 430 repositories.\n",
      "Progress: Found 440 repositories.\n",
      "Progress: Found 450 repositories.\n",
      "Progress: Found 460 repositories.\n",
      "Progress: Found 470 repositories.\n",
      "Progress: Found 480 repositories.\n",
      "Progress: Found 490 repositories.\n",
      "Progress: Found 500 repositories.\n",
      "Progress: Found 510 repositories.\n",
      "Progress: Found 520 repositories.\n",
      "Progress: Found 530 repositories.\n",
      "Progress: Found 540 repositories.\n",
      "Progress: Found 550 repositories.\n",
      "Progress: Found 560 repositories.\n",
      "Progress: Found 570 repositories.\n",
      "Progress: Found 580 repositories.\n",
      "Progress: Found 590 repositories.\n",
      "Progress: Found 600 repositories.\n",
      "Progress: Found 610 repositories.\n",
      "Progress: Found 620 repositories.\n",
      "Progress: Found 630 repositories.\n",
      "Progress: Found 640 repositories.\n",
      "Progress: Found 650 repositories.\n",
      "Progress: Found 660 repositories.\n",
      "Progress: Found 670 repositories.\n",
      "Progress: Found 680 repositories.\n",
      "Progress: Found 690 repositories.\n",
      "Progress: Found 700 repositories.\n",
      "Progress: Found 710 repositories.\n",
      "Progress: Found 720 repositories.\n",
      "Progress: Found 730 repositories.\n",
      "Progress: Found 740 repositories.\n",
      "Progress: Found 750 repositories.\n",
      "Progress: Found 760 repositories.\n",
      "Progress: Found 770 repositories.\n",
      "Progress: Found 780 repositories.\n",
      "Progress: Found 790 repositories.\n",
      "Progress: Found 800 repositories.\n",
      "Progress: Found 810 repositories.\n",
      "Progress: Found 820 repositories.\n",
      "Progress: Found 830 repositories.\n",
      "Progress: Found 840 repositories.\n",
      "Progress: Found 850 repositories.\n",
      "Progress: Found 860 repositories.\n",
      "Progress: Found 870 repositories.\n",
      "Progress: Found 880 repositories.\n",
      "Progress: Found 890 repositories.\n",
      "Progress: Found 900 repositories.\n",
      "Progress: Found 910 repositories.\n",
      "Progress: Found 920 repositories.\n",
      "Progress: Found 930 repositories.\n",
      "Progress: Found 940 repositories.\n",
      "Progress: Found 950 repositories.\n",
      "Progress: Found 960 repositories.\n",
      "Progress: Found 970 repositories.\n",
      "Progress: Found 980 repositories.\n",
      "Progress: Found 990 repositories.\n",
      "Dados exportados para repos.csv com sucesso.\n",
      "Skipping Snailclimb/JavaGuide as it was already cloned.\n",
      "Error running CK on Snailclimb_JavaGuide: Command '['java', '-jar', './ck.jar', './repos/Snailclimb_JavaGuide/', 'true', '0', 'true', './ck_output/Snailclimb_JavaGuide/']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] O sistema não pode encontrar o arquivo especificado",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 297\u001b[0m\n\u001b[0;32m    289\u001b[0m         summary[col] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    290\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmedian(df[col]),\n\u001b[0;32m    291\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(df[col]),\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mstd(df[col])\n\u001b[0;32m    293\u001b[0m         }\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(summary)\n\u001b[1;32m--> 297\u001b[0m final_metrics, repos \u001b[38;5;241m=\u001b[39m retrieve_final_metrics()\n",
      "Cell \u001b[1;32mIn[5], line 262\u001b[0m, in \u001b[0;36mretrieve_final_metrics\u001b[1;34m(quantity)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll previously failed repositories were successfully cloned.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    261\u001b[0m     run_ck_on_repo(repo_owner_name)\n\u001b[1;32m--> 262\u001b[0m     remove_repo(repo_owner_name)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m repos:\n\u001b[0;32m    265\u001b[0m     repo_owner \u001b[38;5;241m=\u001b[39m repo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mowner\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogin\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 197\u001b[0m, in \u001b[0;36mremove_repo\u001b[1;34m(repo_path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_repo\u001b[39m(repo_path):\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun([\n\u001b[0;32m    198\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-rf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./repos/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     ], check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\mp604\\anaconda3\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\mp604\\anaconda3\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\mp604\\anaconda3\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] O sistema não pode encontrar o arquivo especificado"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "token = \"token\"\n",
    "\n",
    "def get_popular_java_repos(total_repos):\n",
    "    url = \"https://api.github.com/graphql\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    all_repos = []\n",
    "    end_cursor = None\n",
    "    max_retries = 5\n",
    "    retry_delay = 5\n",
    "\n",
    "    while len(all_repos) < total_repos:\n",
    "        remaining_repos = total_repos - len(all_repos)\n",
    "        first = min(remaining_repos, 10)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "            search(query: \"language:Java stars:>1\", type: REPOSITORY, first: {first}, after: {f'\"{end_cursor}\"' if end_cursor else 'null'}) {{\n",
    "                edges {{\n",
    "                    node {{\n",
    "                        ... on Repository {{\n",
    "                          name\n",
    "                          url\n",
    "                          stargazers {{\n",
    "                            totalCount\n",
    "                          }}\n",
    "                          owner {{\n",
    "                            login\n",
    "                          }}\n",
    "                          createdAt\n",
    "                          pushedAt\n",
    "                          releases {{\n",
    "                            totalCount\n",
    "                          }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                pageInfo {{\n",
    "                    endCursor\n",
    "                    hasNextPage\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for attempt in range(max_retries):\n",
    "                response = requests.post(url, json={'query': query}, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    search_results = data.get(\"data\", {}).get(\"search\", {})\n",
    "                    if search_results:\n",
    "                        edges = search_results.get(\"edges\", [])\n",
    "                        all_repos.extend(edges)\n",
    "                        end_cursor = search_results.get(\"pageInfo\", {}).get(\"endCursor\")\n",
    "                        if not search_results.get(\"pageInfo\", {}).get(\"hasNextPage\", False):\n",
    "                            return all_repos\n",
    "                        print(f\"Progress: Found {len(all_repos)} repositories.\")\n",
    "                    break\n",
    "                elif response.status_code in [502, 503, 504]:\n",
    "                    print(f\"Error {response.status_code}: Attempt {attempt + 1} of {max_retries}. Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "                elif response.status_code == 429:\n",
    "                    print(\"Rate limit exceeded. Waiting for the reset time.\")\n",
    "                    reset_time = int(response.headers.get('X-RateLimit-Reset', time.time()))\n",
    "                    wait_time = max(reset_time - time.time(), 0)\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to fetch repositories: {response.status_code}\")\n",
    "            else:\n",
    "                raise Exception(\"Max retries reached, aborting.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    return all_repos\n",
    "\n",
    "\n",
    "def calculate_quality_metrics(class_file):\n",
    "    if not os.path.exists(class_file):\n",
    "        print(f\"Arquivo {class_file} não existe.\")\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    if os.stat(class_file).st_size == 0:\n",
    "        print(f\"Arquivo {class_file} está vazio.\")\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    total_cbo = total_dit = total_lcom = loc_total = repo_count = 0\n",
    "    with open(class_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            total_cbo += int(row.get('cbo', 0))\n",
    "            total_dit += int(row.get('dit', 0))\n",
    "            total_lcom += int(row.get('lcom', 0))\n",
    "            loc_total += int(row.get('loc', 0))\n",
    "            repo_count += 1\n",
    "    if repo_count == 0:\n",
    "        print(f\"Nenhum dado encontrado no arquivo {class_file}.\")\n",
    "        return -1, -1, -1, -1\n",
    "    return total_cbo, total_dit, total_lcom, loc_total\n",
    "\n",
    "\n",
    "\n",
    "def export_to_csv(repos, filename=\"repos.csv\"):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Name\", \"URL\", \"Stars\", \"Quantity of Releases\", \"Year Age\", \"Created At\", \"Pushed At\"])\n",
    "        for repo_edge in repos:\n",
    "            repo = repo_edge[\"node\"]\n",
    "            created_at = datetime.datetime.strptime(repo['createdAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            age_years = (datetime.datetime.now() - created_at).days / 365.25\n",
    "            writer.writerow([\n",
    "                repo[\"name\"], repo[\"url\"], \n",
    "                repo[\"stargazers\"][\"totalCount\"],\n",
    "                repo['releases']['totalCount'],\n",
    "                age_years,\n",
    "                repo[\"createdAt\"], \n",
    "                repo[\"pushedAt\"]\n",
    "            ])\n",
    "    print(f\"Dados exportados para {filename} com sucesso.\")\n",
    "\n",
    "\n",
    "def calculate_process_metrics(repos):\n",
    "    process_metrics = []\n",
    "    for repo in repos:\n",
    "        repo_node = repo['node']\n",
    "        created_at = datetime.datetime.strptime(repo_node['createdAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "        age_years = (datetime.datetime.now() - created_at).days / 365.25\n",
    "        releases_count = repo_node['releases']['totalCount']\n",
    "        stargazers_count = repo_node['stargazers']['totalCount']\n",
    "        \n",
    "        process_metrics.append({\n",
    "            'name': repo_node['name'],\n",
    "            \"stars\": stargazers_count,\n",
    "            'age_years': age_years,\n",
    "            'releases_count': releases_count,\n",
    "            'stargazers_count': stargazers_count\n",
    "        })\n",
    "    return process_metrics\n",
    "\n",
    "\n",
    "def merge_metrics(process_metrics, quality_metrics):\n",
    "    merged_data = []\n",
    "    for process_metric in process_metrics:\n",
    "        name = process_metric['name']\n",
    "        quality_metric = quality_metrics.get(name, {})\n",
    "        merged_data.append({\n",
    "            'name': name,\n",
    "            'popularity': process_metric['stars'],\n",
    "            'maturity': process_metric['age_years'],\n",
    "            \"activity\": process_metric['releases_count'],\n",
    "            'loc_total': quality_metric['loc_total'],\n",
    "            'avg_cbo': quality_metric['avg_cbo'],\n",
    "            'avg_dit': quality_metric['avg_dit'],\n",
    "            'avg_lcom': quality_metric['avg_lcom']\n",
    "        })\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def clone_repo(repo_url, repo_name, repo_owner, failed_repos=[]):\n",
    "    try:\n",
    "        repo_path = f\"./repos/{repo_owner}_{repo_name}\"\n",
    "        ck_output = f\"./ck_output/{repo_owner}_{repo_name}\"\n",
    "        os.makedirs(os.path.dirname(repo_path), exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(ck_output):\n",
    "            print(f\"Skipping {repo_owner}/{repo_name} as it was already cloned.\")\n",
    "            return False\n",
    "        subprocess.run([\n",
    "            \"git\", \"clone\", \"--single-branch\", \"--no-tags\", \"--depth\", \"1\", repo_url, repo_path\n",
    "        ], check=True)\n",
    "        print(f\"Cloned {repo_owner}/{repo_name} successfully ({repo_url}).\")\n",
    "        return True \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cloning {repo_owner}/{repo_name} -> {repo_url}: {e}.\")\n",
    "        failed_repos.append((repo_name, repo_url, repo_owner))\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_result_dir(repo_path):\n",
    "  try:\n",
    "    output_dir = f\"./ck_output/{repo_path}/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "  except Exception as e:\n",
    "    print(f\"Error running the creation directories: {e}\")\n",
    "\n",
    "\n",
    "def remove_repo(repo_path):\n",
    "  try:\n",
    "    subprocess.run([\n",
    "      \"rm\", \"-rf\", f\"./repos/{repo_path}\"\n",
    "    ], check=True)\n",
    "    print(f\"Removed {repo_path} successfully.\")\n",
    "  except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error removing {repo_path}: {e}\")\n",
    "    \n",
    "\n",
    "def remove_all_folders():\n",
    "  try:\n",
    "    subprocess.run([\n",
    "      \"rm\", \"-rf\", \"./repos\"\n",
    "    ], check=True)\n",
    "    subprocess.run([\n",
    "      \"rm\", \"-rf\", \"./ck_output\"\n",
    "    ], check=True)\n",
    "    print(f\"Removed all folders successfully.\")\n",
    "  except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error removing all folders: {e}\")\n",
    "\n",
    "\n",
    "def run_ck_on_repo(repo_path):\n",
    "    try:\n",
    "        repo_path_url = f\"./repos/{repo_path}/\"\n",
    "        output_dir = f\"./ck_output/{repo_path}/\"\n",
    "        if os.path.exists(output_dir) and os.path.exists(f\"{output_dir}/class.csv\"):\n",
    "            print(f\"Skipping CK on {repo_path} as it was already run.\")\n",
    "            return\n",
    "        subprocess.run([\n",
    "            \"java\", \"-jar\", \"./ck.jar\",\n",
    "            repo_path_url, \"true\", \"0\", \"true\", output_dir\n",
    "        ], check=True)\n",
    "        print(f\"Ran CK on {repo_path} successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running CK on {repo_path}: {e}\")\n",
    "\n",
    "\n",
    "def retrieve_final_metrics(quantity=1000):\n",
    "    repos = get_popular_java_repos(quantity)\n",
    "    export_to_csv(repos)\n",
    "\n",
    "    quality_metrics = {}\n",
    "    for repo in repos:\n",
    "        repo_url = repo[\"node\"][\"url\"]\n",
    "        repo_name = repo[\"node\"][\"name\"]\n",
    "        repo_owner = repo[\"node\"][\"owner\"][\"login\"]\n",
    "        \n",
    "        repo_owner_name = f\"{repo_owner}_{repo_name}\"\n",
    "        create_result_dir(repo_owner_name)\n",
    "\n",
    "        failed_repos = []\n",
    "        clone_repo(repo_url, repo_name, repo_owner, failed_repos=failed_repos)\n",
    "        if failed_repos:\n",
    "            print(\"\\nRetrying failed repositories...\\n\")\n",
    "            retry_failed_repos = []\n",
    "            for repo_name, repo_url, repo_owner in failed_repos:\n",
    "                clone_repo(repo_url, repo_name, repo_owner, failed_repos=retry_failed_repos)\n",
    "\n",
    "            if retry_failed_repos:\n",
    "                print(\"\\nThe following repositories could not be cloned even after retries:\")\n",
    "                for repo_name, repo_url, repo_owner in retry_failed_repos:\n",
    "                    print(f\"Failed: {repo_name} ({repo_url})\")\n",
    "            else:\n",
    "                print(\"\\nAll previously failed repositories were successfully cloned.\")\n",
    "        run_ck_on_repo(repo_owner_name)\n",
    "        remove_repo(repo_owner_name)\n",
    "    \n",
    "    for repo in repos:\n",
    "        repo_owner = repo['node']['owner']['login']\n",
    "        repo_name = repo['node']['name']\n",
    "        class_file = f\"./ck_output/{repo_owner}_{repo_name}/class.csv\"\n",
    "        quality_metrics_result = calculate_quality_metrics(class_file)\n",
    "        if quality_metrics_result:\n",
    "            avg_cbo, avg_dit, avg_lcom, loc_total = quality_metrics_result\n",
    "            quality_metrics[repo_name] = {\n",
    "                'avg_cbo': avg_cbo,\n",
    "                'avg_dit': avg_dit,\n",
    "                'avg_lcom': avg_lcom,\n",
    "                'loc_total': loc_total\n",
    "            }\n",
    "            print(quality_metrics[repo_name])\n",
    "        else:\n",
    "            print(f\"Ignoring {repo_name} due to missing or empty quality metrics.\")\n",
    "\n",
    "    process_metrics = calculate_process_metrics(repos)\n",
    "    final_metrics = merge_metrics(process_metrics, quality_metrics)\n",
    "    return final_metrics, repos\n",
    "\n",
    "\n",
    "def summarize_data(df, columns):\n",
    "    summary = {}\n",
    "    for col in columns:\n",
    "        summary[col] = {\n",
    "            'median': np.median(df[col]),\n",
    "            'mean': np.mean(df[col]),\n",
    "            'std': np.std(df[col])\n",
    "        }\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "final_metrics, repos = retrieve_final_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: Found 10 repositories.\n",
      "Progress: Found 20 repositories.\n",
      "Progress: Found 30 repositories.\n",
      "Progress: Found 40 repositories.\n",
      "Progress: Found 50 repositories.\n",
      "Progress: Found 60 repositories.\n",
      "Progress: Found 70 repositories.\n",
      "Progress: Found 80 repositories.\n",
      "Progress: Found 90 repositories.\n",
      "Progress: Found 100 repositories.\n",
      "Progress: Found 110 repositories.\n",
      "Progress: Found 120 repositories.\n",
      "Progress: Found 130 repositories.\n",
      "Progress: Found 140 repositories.\n",
      "Progress: Found 150 repositories.\n",
      "Progress: Found 160 repositories.\n",
      "Progress: Found 170 repositories.\n",
      "Progress: Found 180 repositories.\n",
      "Progress: Found 190 repositories.\n",
      "Progress: Found 200 repositories.\n",
      "Progress: Found 210 repositories.\n",
      "Progress: Found 220 repositories.\n",
      "Progress: Found 230 repositories.\n",
      "Progress: Found 240 repositories.\n",
      "Progress: Found 250 repositories.\n",
      "Progress: Found 260 repositories.\n",
      "Progress: Found 270 repositories.\n",
      "Progress: Found 280 repositories.\n",
      "Progress: Found 290 repositories.\n",
      "Progress: Found 300 repositories.\n",
      "Progress: Found 310 repositories.\n",
      "Progress: Found 320 repositories.\n",
      "Progress: Found 330 repositories.\n",
      "Progress: Found 340 repositories.\n",
      "Progress: Found 350 repositories.\n",
      "Progress: Found 360 repositories.\n",
      "Progress: Found 370 repositories.\n",
      "Progress: Found 380 repositories.\n",
      "Progress: Found 390 repositories.\n",
      "Progress: Found 400 repositories.\n",
      "Progress: Found 410 repositories.\n",
      "Progress: Found 420 repositories.\n",
      "Progress: Found 430 repositories.\n",
      "Progress: Found 440 repositories.\n",
      "Progress: Found 450 repositories.\n",
      "Progress: Found 460 repositories.\n",
      "Progress: Found 470 repositories.\n",
      "Progress: Found 480 repositories.\n",
      "Progress: Found 490 repositories.\n",
      "Progress: Found 500 repositories.\n",
      "Progress: Found 510 repositories.\n",
      "Progress: Found 520 repositories.\n",
      "Progress: Found 530 repositories.\n",
      "Progress: Found 540 repositories.\n",
      "Progress: Found 550 repositories.\n",
      "Progress: Found 560 repositories.\n",
      "Progress: Found 570 repositories.\n",
      "Progress: Found 580 repositories.\n",
      "Progress: Found 590 repositories.\n",
      "Progress: Found 600 repositories.\n",
      "Progress: Found 610 repositories.\n",
      "Progress: Found 620 repositories.\n",
      "Progress: Found 630 repositories.\n",
      "Progress: Found 640 repositories.\n",
      "Progress: Found 650 repositories.\n",
      "Progress: Found 660 repositories.\n",
      "Progress: Found 670 repositories.\n",
      "Progress: Found 680 repositories.\n",
      "Progress: Found 690 repositories.\n",
      "Progress: Found 700 repositories.\n",
      "Progress: Found 710 repositories.\n",
      "Progress: Found 720 repositories.\n",
      "Progress: Found 730 repositories.\n",
      "Progress: Found 740 repositories.\n",
      "Progress: Found 750 repositories.\n",
      "Progress: Found 760 repositories.\n",
      "Progress: Found 770 repositories.\n",
      "Progress: Found 780 repositories.\n",
      "Progress: Found 790 repositories.\n",
      "Progress: Found 800 repositories.\n",
      "Progress: Found 810 repositories.\n",
      "Progress: Found 820 repositories.\n",
      "Progress: Found 830 repositories.\n",
      "Progress: Found 840 repositories.\n",
      "Progress: Found 850 repositories.\n",
      "Progress: Found 860 repositories.\n",
      "Progress: Found 870 repositories.\n",
      "Progress: Found 880 repositories.\n",
      "Progress: Found 890 repositories.\n",
      "Progress: Found 900 repositories.\n",
      "Progress: Found 910 repositories.\n",
      "Progress: Found 920 repositories.\n",
      "Progress: Found 930 repositories.\n",
      "Progress: Found 940 repositories.\n",
      "Progress: Found 950 repositories.\n",
      "Progress: Found 960 repositories.\n",
      "Progress: Found 970 repositories.\n",
      "Progress: Found 980 repositories.\n",
      "Progress: Found 990 repositories.\n",
      "Dados exportados para repos.csv com sucesso.\n",
      "Skipping Snailclimb/JavaGuide as it was already cloned.\n",
      "Error running CK on Snailclimb_JavaGuide: Command '['java', '-jar', './ck.jar', './repos/Snailclimb_JavaGuide/', 'true', '0', 'true', './ck_output/Snailclimb_JavaGuide/']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] O sistema não pode encontrar o arquivo especificado",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_metrics, repos \u001b[38;5;241m=\u001b[39m retrieve_final_metrics()\n\u001b[0;32m      2\u001b[0m final_metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(final_metrics)\n\u001b[0;32m      3\u001b[0m metrics_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopularity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaturity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc_total\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_cbo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_dit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_lcom\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 262\u001b[0m, in \u001b[0;36mretrieve_final_metrics\u001b[1;34m(quantity)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll previously failed repositories were successfully cloned.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    261\u001b[0m     run_ck_on_repo(repo_owner_name)\n\u001b[1;32m--> 262\u001b[0m     remove_repo(repo_owner_name)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m repos:\n\u001b[0;32m    265\u001b[0m     repo_owner \u001b[38;5;241m=\u001b[39m repo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mowner\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogin\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 197\u001b[0m, in \u001b[0;36mremove_repo\u001b[1;34m(repo_path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_repo\u001b[39m(repo_path):\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun([\n\u001b[0;32m    198\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-rf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./repos/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     ], check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\mp604\\anaconda3\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\mp604\\anaconda3\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\mp604\\anaconda3\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] O sistema não pode encontrar o arquivo especificado"
     ]
    }
   ],
   "source": [
    "final_metrics, repos = retrieve_final_metrics()\n",
    "final_metrics_df = pd.DataFrame(final_metrics)\n",
    "metrics_columns = ['popularity', 'maturity', 'activity', 'loc_total', 'avg_cbo', 'avg_dit', 'avg_lcom']\n",
    "summary_df = summarize_data(final_metrics_df, metrics_columns)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(final_metrics)\n\u001b[0;32m      2\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_cbo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_dit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_lcom\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m      3\u001b[0m correlation \u001b[38;5;241m=\u001b[39m df_filtered[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopularity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_cbo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_dit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_lcom\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcorr()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(final_metrics)\n",
    "df_filtered = df[(df['avg_cbo'] != -1) & (df['avg_dit'] != -1) & (df['avg_lcom'] != -1)]\n",
    "correlation = df_filtered[['popularity', 'avg_cbo', 'avg_dit', 'avg_lcom']].corr()\n",
    "\n",
    "print(\"Matriz de Correlação:\")\n",
    "print(correlation)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Gráfico de dispersão para CBO\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(df_filtered['popularity'], df_filtered['avg_cbo'], alpha=0.5)\n",
    "plt.title('Popularity vs. Avg CBO')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Avg CBO')\n",
    "\n",
    "# Gráfico de dispersão para DIT\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(df_filtered['popularity'], df_filtered['avg_dit'], alpha=0.5)\n",
    "plt.title('Popularity vs. Avg DIT')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Avg DIT')\n",
    "\n",
    "# Gráfico de dispersão para LCOM\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(df_filtered['popularity'], df_filtered['avg_lcom'], alpha=0.5)\n",
    "plt.title('Popularity vs. Avg LCOM')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Avg LCOM')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_metrics)\n",
    "df_filtered = df[(df['avg_cbo'] != -1) & (df['avg_dit'] != -1) & (df['avg_lcom'] != -1)]\n",
    "correlation = df_filtered[['maturity', 'avg_cbo', 'avg_dit', 'avg_lcom']].corr()\n",
    "\n",
    "print(\"Matriz de Correlação:\")\n",
    "print(correlation)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Gráfico de dispersão para CBO\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(df_filtered['maturity'], df_filtered['avg_cbo'], alpha=0.5)\n",
    "plt.title('Maturity vs. Avg CBO')\n",
    "plt.xlabel('Maturity')\n",
    "plt.ylabel('Avg CBO')\n",
    "\n",
    "# Gráfico de dispersão para DIT\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(df_filtered['maturity'], df_filtered['avg_dit'], alpha=0.5)\n",
    "plt.title('Maturity vs. Avg DIT')\n",
    "plt.xlabel('Maturity')\n",
    "plt.ylabel('Avg DIT')\n",
    "\n",
    "# Gráfico de dispersão para LCOM\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(df_filtered['maturity'], df_filtered['avg_lcom'], alpha=0.5)\n",
    "plt.title('Maturity vs. Avg LCOM')\n",
    "plt.xlabel('Maturity')\n",
    "plt.ylabel('Avg LCOM')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_metrics)\n",
    "df_filtered = df[(df['avg_cbo'] != -1) & (df['avg_dit'] != -1) & (df['avg_lcom'] != -1)]\n",
    "correlation = df_filtered[['activity', 'avg_cbo', 'avg_dit', 'avg_lcom']].corr()\n",
    "\n",
    "print(\"Matriz de Correlação:\")\n",
    "print(correlation)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Gráfico de dispersão para CBO\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(df_filtered['activity'], df_filtered['avg_cbo'], alpha=0.5)\n",
    "plt.title('Activity vs. Avg CBO')\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Avg CBO')\n",
    "\n",
    "# Gráfico de dispersão para DIT\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(df_filtered['activity'], df_filtered['avg_dit'], alpha=0.5)\n",
    "plt.title('Activity vs. Avg DIT')\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Avg DIT')\n",
    "\n",
    "# Gráfico de dispersão para LCOM\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(df_filtered['activity'], df_filtered['avg_lcom'], alpha=0.5)\n",
    "plt.title('Activity vs. Avg LCOM')\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Avg LCOM')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_metrics)\n",
    "df_filtered = df[(df['avg_cbo'] != -1) & (df['avg_dit'] != -1) & (df['avg_lcom'] != -1)]\n",
    "correlation = df_filtered[['loc_total', 'avg_cbo', 'avg_dit', 'avg_lcom']].corr()\n",
    "\n",
    "print(\"Matriz de Correlação:\")\n",
    "print(correlation)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Gráfico de dispersão para CBO\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(df_filtered['loc_total'], df_filtered['avg_cbo'], alpha=0.5)\n",
    "plt.title('LOC Total vs. Avg CBO')\n",
    "plt.xlabel('LOC Total')\n",
    "plt.ylabel('Avg CBO')\n",
    "\n",
    "# Gráfico de dispersão para DIT\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(df_filtered['loc_total'], df_filtered['avg_dit'], alpha=0.5)\n",
    "plt.title('LOC Total vs. Avg DIT')\n",
    "plt.xlabel('LOC Total')\n",
    "plt.ylabel('Avg DIT')\n",
    "\n",
    "# Gráfico de dispersão para LCOM\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(df_filtered['loc_total'], df_filtered['avg_lcom'], alpha=0.5)\n",
    "plt.title('LOC Total vs. Avg LCOM')\n",
    "plt.xlabel('LOC Total')\n",
    "plt.ylabel('Avg LCOM')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
